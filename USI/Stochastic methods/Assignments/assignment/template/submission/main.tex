\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}

\usepackage{assignment}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{theorem}
\usepackage{algorithm,algorithmic}
\hyphenation{PageRank}
\hyphenation{PageRanks}

\newtheorem{theorem}{Theorem}
\renewcommand\thesubsection{\arabic{subsection}}
\DeclareOldFontCommand{\bf}{\normalfont\bfseries}{\mathbf}
\DeclareOldFontCommand{\it}{\normalfont\bfseries}{\mathit}


\allowdisplaybreaks

\begin{document}

\setassignment
\setduedate{Friday, May 14, 2021, 11.59pm}
\serieheader{Stochastic Methods}{Academic Year 2020/2021}{Prof. Dr. Illia Horenko (illia.horenko@usi.ch)}{Edoardo Vecchi (edoardo.vecchi@usi.ch)}{Assignment 4 - Solution}{[Ashutosh Singh]}

%-----------------------------------------------------------------------------------------------

\section*{Exercise 1: Inconsistent Systems of Equations}

\begin{itemize}
	\item [(a)]
	\begin{equation}
	    A_1x = b_1
	\end{equation}
	\[
        \begin{bmatrix}
        1 & 0 \\
        1 & 0 \\
        1 & 0 \\
        \end{bmatrix}
        \begin{bmatrix}
        x_1 \\ x_2 \\ 
        \end{bmatrix}
        =
        \begin{bmatrix}
        5 \\ 2 \\ 4
        \end{bmatrix}
    \]
    
    \begin{equation}
	    A_1^TA_1x = A_1^Tb_1
	\end{equation}
    \[
    	\begin{bmatrix}
            1 & 1 & 1 \\
            0 & 0 & 0\\
        \end{bmatrix}
        \begin{bmatrix}
            1 & 0 \\
            1 & 0 \\
            1 & 0 \\
        \end{bmatrix}
        \begin{bmatrix}
        x_1 \\ x_2 \\ 
        \end{bmatrix}
        =
        \begin{bmatrix}
            1 & 1 & 1 \\
            0 & 0 & 0\\
        \end{bmatrix}
        \begin{bmatrix}
        5 \\ 2 \\ 4
        \end{bmatrix}
    \]
    \[
        \begin{bmatrix}
        3 & 0 \\
        0 & 0 \\
        \end{bmatrix}
        \begin{bmatrix}
        x_1 \\ x_2 \\ 
        \end{bmatrix}
        =
        \begin{bmatrix}
        11 \\ 0
        \end{bmatrix}
    \]
    \begin{equation}
	    \Rightarrow x_1^* = \frac{11}{3};
	    \\
	    \Rightarrow x_2^* = k
	\end{equation}
    
	{Residual}\\
	 \begin{equation}
	    r = b_1 - A_1x^*
	 \end{equation}
	 
	 \[
	    r = 
	    \begin{bmatrix}
        5 \\ 2 \\ 4
        \end{bmatrix}
        -
        \begin{bmatrix}
        \frac{11}{3} \\
        \frac{11}{3} \\
        \frac{11}{3} \\
        \end{bmatrix}
    \]
    \[
	    r = 
        \begin{bmatrix}
        \frac{4}{3} \\
        -\frac{5}{3} \\
        \frac{1}{3} \\
        \end{bmatrix}
    \]
    
    {Euclidean norm of Residual}\\
     \begin{equation}
	    ||r||_2 = \sqrt{(\frac{4}{3})^2 + (-\frac{5}{3})^2 + (\frac{1}{3})^2}
	 \end{equation}
	 \begin{equation}
	    ||r||_2 = 2.16
	 \end{equation}
	 
    
    {SE of Residual}\\
    \begin{equation}
	    SE = ||r||_2^2
	 \end{equation}
	 \begin{equation}
	    SE = 4.67
	 \end{equation}
    
    {RMSE of Residual}\\
    \begin{equation}
	    RMSE = \sqrt{(\frac{SE}{m})}
	 \end{equation}
	 \begin{equation}
	    RMSE = \sqrt{(\frac{4.67}{3})} = 1.247
	 \end{equation}
	
	
	\item [(b)]
	
	\begin{equation}
	    A_2x = b_2
	\end{equation}
	\[
        \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 1 \\
        1 & 2 & 1 \\
        1 & 0 & 1 \\
        \end{bmatrix}
        \begin{bmatrix}
        x_1 \\ x_2 \\ x_3 \\
        \end{bmatrix}
        =
        \begin{bmatrix}
        2 \\ 2 \\ 3 \\ 4
        \end{bmatrix}
    \]
    
    \begin{equation}
	    A_2^TA_2x = A_2^Tb_2
	\end{equation}
    \[
    	\begin{bmatrix}
            1 & 0 & 1 & 1 \\
            0 & 1 & 2 & 0 \\
            0 & 1 & 1 & 1 \\
        \end{bmatrix}
        \begin{bmatrix}
           1 & 0 & 0 \\
            0 & 1 & 1 \\
            1 & 2 & 1 \\
            1 & 0 & 1 \\
        \end{bmatrix}
        \begin{bmatrix}
            x_1 \\ x_2 \\ x_3 \\
        \end{bmatrix}
        =
        \begin{bmatrix}
            1 & 0 & 1 & 1 \\
            0 & 1 & 2 & 0 \\
            0 & 1 & 1 & 1 \\
        \end{bmatrix}
        \begin{bmatrix}
            2 \\ 2 \\ 3 \\ 4
        \end{bmatrix}
    \]
    \[
        \begin{bmatrix}
        3 & 3 & 2\\
        3 & 6 & 3 \\
        2 & 3 & 3 \\
        \end{bmatrix}
        \begin{bmatrix}
        x_1 \\ x_2 \\ x_3 \\
        \end{bmatrix}
        =
        \begin{bmatrix}
        9 \\ 10 \\ 9
        \end{bmatrix}
    \]
    
    {After gaussian elimination}
    \[
        \begin{bmatrix}
        3 & 3 & 2\\
        0 & 3 & 1 \\
        0 & 0 & \frac{4}{3} \\
        \end{bmatrix}
        \begin{bmatrix}
        x_1^* \\ x_2^* \\ x_3^* \\
        \end{bmatrix}
        =
        \begin{bmatrix}
        9 \\ 1 \\ \frac{8}{3}
        \end{bmatrix}
    \]
    \begin{equation}
	    \Rightarrow x_1^* = 2;
	    \\
	    \Rightarrow x_2^* = -\frac{1}{3};
	    \\
	    \Rightarrow x_3^* = 2
	\end{equation}
	
	
	{Residual}\\
	 \begin{equation}
	    r = b_2 - A_2x^*
	 \end{equation}
	 
	 \[
	    r = 
	    \begin{bmatrix}
        2 \\ 2 \\ 3 \\ 4
        \end{bmatrix}
        -
        \begin{bmatrix}
        \frac{5}{3} \\
        \frac{5}{3} \\
        \frac{10}{3} \\
        4 \\
        \end{bmatrix}
    \]
    \[
	    r = 
        \begin{bmatrix}
        \frac{1}{3} \\
        \frac{1}{3} \\
        -\frac{1}{3} \\
        0
        \end{bmatrix}
    \]
    
    {Euclidean norm of Residual}\\
     \begin{equation}
	    ||r||_2 = \sqrt{(\frac{1}{3})^2 + (\frac{1}{3})^2 + (-\frac{1}{3})^2 + 0}
	 \end{equation}
	 \begin{equation}
	    ||r||_2 = 0.577
	 \end{equation}
	 
    
    {SE of Residual}\\
    \begin{equation}
	    SE = ||r||_2^2
	 \end{equation}
	 \begin{equation}
	    SE = \frac{1}{3}
	 \end{equation}
    
    {RMSE of Residual}\\
    \begin{equation}
	    RMSE = \sqrt{(\frac{SE}{m})}
	 \end{equation}
	 \begin{equation}
	    RMSE = \sqrt{(\frac{\frac{1}{3}}{4})} = \sqrt{\frac{1}{12}}
	 \end{equation}
\end{itemize}

%------------------------------------------------------------------------------------

\section*{Exercise 2: Comparison of Polynomials Models for Least Squares}

\begin{itemize}
	\item [(a)] 
	{1a}
	\begin{center}
        \begin{tabular}{ c c c c}
         Method/metric & norm of residual & SE & RMSE\\ 
         hand & 2.16 & 4.67 & 1.247\\ 
         leastSquares method & 2.1602 & 4.6667 & 1.2472\\  
        \end{tabular}
    \end{center}
    \\
    {1b}
	\begin{center}
        \begin{tabular}{ c c c c}
         Method/metric & norm of residual & SE & RMSE\\ 
         hand & 0.577 & 0.33 & 0.288\\ 
         leastSquares method & 0.5774 & 0.3333 & 0.2887\\  
        \end{tabular}
    \end{center}
	\item [(b)] Figures in figures folder under exercise-2b folder
	\item [(c)] Figures in figures folder under exercise-2c folder
	\item [(d)] Figures in figures folder under exercise-2d folder
	\item [(e)] Comparision of linear, quadratic and cubic polynomial models
	
	\begin{center}
    {\bf{Kerosene Dataset}}
    \end{center}
	\begin{center}
	    
        \begin{tabular}{ c c c c c c}
         model/metric & norm of residual & SE & RMSE & predicted 2012 & true 2012\\ 
         linear & 2.11 & 4.46 & 0.37 & 208.476 & 267.89\\ 
         quadratic & 2.04 & 4.19 & 0.367 & 224.916  & 267.89\\  
         cubic & 2.66 & 7.12  & 0.4793 & 329.728 & 267.89
        \end{tabular}
    \end{center}
    
    {For both the datasets we clearly see that All metrics are better for quadratic model and prediction on unseen data is closest as well. I would chose quadratic model}
    
    \begin{center}
    {\bf{Crude Oil Dataset}}
    \end{center}
    \\
    \\
    \begin{center}
        \begin{tabular}{ c c c c c c}
         model/metric & norm of residual & SE & RMSE & predicted 2012 & true 2012\\ 
         linear & 1.96 & 3.85 & 0.35 & 1.707e+04 & 1.311e+04\\ 
         quadratic & 1.57 & 2.49 & 0.28 & 1.40e+04 & 1.311e+04\\  
         cubic & 2.51 & 6.32  & 0.4517 & 4.190e+03 & 1.311e+04
        \end{tabular}
    \end{center}
    
    {For crude-oil dataset we can see that all error metrics improve as we increase the degree of the polynomial. But our prediction on unseen values keeps getting worse.
    Given these conditions without any more test data I would select linear model}
\end{itemize}

%-----------------------------------------------------------------------------------------

\section*{Exercise 3: Analysis of Periodic Data}

\begin{itemize}
	\item [(a)] periodicA
	    
    \begin{center}
        \begin{tabular}{ c c c c}
         model/metric & norm of residual & SE & RMSE\\ 
         1960-63 & 10.08 & 101.68 & 1.68\\ 
         1960-70 & 18.001 & 324.04 & 1.64\\ 
        \end{tabular}
    \end{center}
    
    
	\item [(b)] periodicB
	\begin{center}
        \begin{tabular}{ c c c c}
         model/metric & norm of residual & SE & RMSE\\ 
         1960-63 & 9.49 & 90.17 & 1.58\\ 
         1960-70 & 17.50 & 306.29 & 1.59\\ 
        \end{tabular}
    \end{center}
	\item [(c)]
	{After running predictions on entire dataset I could comeup with the result that adding more data has made the model the better.}\\
	
	{Lack of time and upcoming project presentations I was not able to include more details in the report}
	
\end{itemize}

%-----------------------------------------------------------------------------------------

\section*{Exercise 4: Linearization and Levenberg-Marquardt Method for Exponential Model}

\begin{itemize}
	\item [(a)]
	\begin{equation}
	y_i = \alpha_1x_i^\alpha_2
	\end{equation}
	{Applying log on bot sides}\\
	
	\begin{equation}
	log(y) = log(\alpha_1) + \alpha_2log(x)
	log(y) = k_1 + \alpha_2log(x)
	\end{equation}
	

	{From nuclear dataset}\\
	
	\[ years = 
	\begin{bmatrix}
	1999  \\
	2000  \\
	2001  \\
	2002  \\
	2003  \\
	2004  \\
	2005  \\
	2006
	\end{bmatrix}
	\]
	
	\[  consumption = 
        \begin{bmatrix}
        14.09 \\
        15.90 \\
     	16.60 \\
        25.17 \\
        41.66 \\
        47.95 \\
        50.33 \\
        54.85 \\
        \end{bmatrix}
    \]
    \\
    \[  change = 
        \begin{bmatrix}
        4.68 \\
        12.85 \\
        4.40 \\
        51.63 \\
        65.51 \\
        15.10 \\
        4.96 \\
        8.98 \\

        \end{bmatrix}
    \]
    
    {Scaling years by diving them minimum value}\\
    {Writing system in form Ax=b}\\
	
	\[
        \begin{bmatrix}
        1 & ln(\frac{1999}{1999}) \\
        1 & ln(\frac{2000}{1999}) \\
     	1 & ln(\frac{2001}{1999}) \\
        1 & ln(\frac{2002}{1999}) \\
        1 & ln(\frac{2003}{1999}) \\
        1 & ln(\frac{2004}{1999}) \\
        1 & ln(\frac{2005}{1999}) \\
        1 & ln(\frac{2006}{1999}) \\
        \end{bmatrix}
        \begin{bmatrix}
        k_1 \\ \alpha_2 \\
        \end{bmatrix}
        =
        \begin{bmatrix}
        ln(14.09) \\
        ln(15.90) \\
     	ln(16.60) \\
        ln(25.17) \\
        ln(41.66) \\
        ln(47.95) \\
        ln(50.33) \\
        ln(54.85) \\
        \end{bmatrix}
    \]
    \\
    {Solving this system by leastSqaures we get}\\
	
	\begin{equation}
	RMSE_{log} = 0.1446
	\end{equation}
	
	\begin{equation}
	RMSE_{exp} = 10.9191
	\end{equation}
	
	
	
	
	\item [(b)] {Steps} \\
	    \begin{itemize}
          \item A function to calculate residual vector at given paramater values
          \item A function to calculate Jacobian. Precaculated partial derivatives. This function just returns Jacobian Matrix at particular parameter values .
          \item Run Levenberg-Marquardt algorithm for 10000 steps. to obtain the RMSE.
        \end{itemize}
		\begin{equation}
	        RMSE_{LM} = 0.6205
	   \end{equation}
	   NOTE: This RMSE is by using year column as index rather that values. Example
	   	\[ years = 
        	\begin{bmatrix}
        	1  \\
        	2  \\
        	3  \\
        	4  \\
        	5  \\
        	6  \\
        	7  \\
        	8
        	\end{bmatrix}
    	\]
    	
    	And I scaled consumption values using min-max scalar.
    	
    	{With original settings i.e. min scaled years vector and no scaling on consumption}
    	\begin{equation}
	        RMSE_{LM} = 4.989
	   \end{equation}
	\item [(c)]
	I would chose LM model because it has lower RMSE than log linearized model
\end{itemize}


%-----------------------------------------------------------------------------------------

\section*{Exercise 5: Tikhonov Regularization}

\begin{itemize}
	\item [(a)] {Linear least squares with Tikhonov regularization}
	\begin{equation}
	    \min_x ||{Ax - b}||^2 + \alpha||{x}||^2
	\end{equation}
	\begin{equation}
	    J =  (Ax - b)^T(Ax - b) + \alpha(x^Tx)
	\end{equation}
	{Differentiating w.r.t x for finding the optimal point}
	
	\begin{equation}
	    \frac{\partial J}{\partial x} =  \frac{\partial}{\partial x}{ ((Ax - b)^T(Ax - b) + \alpha(x^Tx))}
	 
	\end{equation}
	
	\begin{equation}
	    \frac{\partial J}{\partial x} =  \frac{\partial}{\partial x}{ (x^TA^TAx - 2 (Ax)^Tb - b^Tb + \alpha(x^Tx))}
	 
	\end{equation}
	
	{Putting}
	\begin{equation}
	    \frac{\partial J}{\partial x} = 0
	\end{equation}
	
	\begin{equation}
	    2A^TAx - 2A^Tb + 2\alpha x = 0
	\end{equation}
	
	\begin{equation}
	    x^* = (A^TA + \alpha)^{-1}A^Tb
	\end{equation}
	
	{Purpose of the regularization parameter alpha is that it forces the weights towards zero( but not exactly zero in this case)}
	\\ 
	{Reasonable values of alpha lie between 0 and 0.1}
	
	\item [(b)] Figures are under figures/exercise-5b
	\item [(c)] Figures are under figures/exercise-5c
	
	{I chose values of alpha between 0 and 0.1}
\end{itemize}


%-----------------------------------------------------------------------------------------


\end{document}
